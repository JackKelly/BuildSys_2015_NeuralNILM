<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>NeuralNILM</title>

    <meta name="description" content="Deep Neural Networks applied to Energy Disaggregation">
    <meta name="author" content="Jack Kelly">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="bower_components/reveal.js/css/reveal.css">
    <link rel="stylesheet" href="bower_components/reveal.js/css/theme/black.css" id="theme">
    <link rel="stylesheet" href="css/jack.css">
    
    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'bower_components/reveal.js/css/print/pdf.css' : 'bower_components/reveal.js/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h1>Neural NILM</h1>
          <h3>Deep Neural Networks Applied To<br>Energy
            Disaggregation</h3>
          <p>
            <small>
              <a href="http://jack-kelly.com">Jack Kelly</a>
              &amp; <a href="http://www.doc.ic.ac.uk/~wjk/">William
                Knottenbelt</a>
              <br>
              Imperial College London              
          </small></p>
          <p>
            <small>

            </small>
          </p>
        </section>

        <section id="outline">
          <h1>Outline</h1>
          <ol style="width:70%">
            <li class="fragment">Why use deep neural nets (DNNs) for
              NILM?</li>
            <!-- Describe *what* DNNs are good at. -->
            <li class="fragment">How DNNs work</li>            
            <li class="fragment">Three DNN architectures for NILM</li>
            <li class="fragment">Data augmentation</li>
            <li class="fragment">Results</li>
            <li class="fragment">Conclusions &amp; future work</li>            
          </ol>
        </section>

        <!-- ******************************************************
        In this section, describe *what* DNNs are good at (not how). 
        ******************************************************** -->
        
        <section id="why-dnns-for-nilm">
          <h1 class="dim">Outline</h1>
          <ol style="width:70%">
            <li>Why use deep neural nets (DNNs) for NILM?</li>
            <li class="dim">How DNNs work</li>            
            <li class="dim">Three DNN architectures for NILM</li>
            <li class="dim">Data augmentation</li>            
            <li class="dim">Results</li>
            <li class="dim">Conclusions &amp; future work</li>            
          </ol>
        </section>        
        
        <section id="washer">
          <h1>Name the Appliance?</h1>
          <p class="placeholder"></p>
          <span class="fragment"></span>
          <span class="fragment"></span>
          <!-- 
               TODO: 
               Tidy the x-axis ticks
               -->
        </section>

        <section>
          <h1>Face Recognition</h2>
          <h2 class="fragment">Manual Feature Extraction</h3>
        </section>

        <section>
          <img src="images/hart1.jpg">
          <p class="tiny fragment">
            <a href="http://www.georgehart.com/research/hartbiog.html">
              georgehart.com/research/hartbiog.html</a>
          </p>
          <!--- D3 TODO (medium priority)
              Animate detecting eyes, nose, mouth, measuring distances.
              --->
        </section>

        <section>
          <img src="images/hart2.jpg" height=600>
          <p class="tiny">
            <a href="http://scgp.stonybrook.edu/archives/8516">
              scgp.stonybrook.edu/archives/8516</a>
          </p>
        </section>

        <section>
          <h1>SIFT</h1>
          <p>Scale-Invariant Feature Transform</p>
          <img src="images/sift_keypoints.jpg">
          <p class="tiny">
            Image from: <a href="http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html">
              OpenCV-Python Tutorials: Introduction to SIFT</a>
          </p>
          <p class="tiny">
              David G. Lowe (1999) Object recognition from local
              scale-invariant
              features. ICCV'99. DOI:<a href="http://dx.doi.org/10.1109/ICCV.1999.790410">10.1109/ICCV.1999.790410</a>
              <br>Cited by 10,106 papers!
          </p>          
        </section>

        <section>
          <h1>Deep Neural Nets</h1>
          <h2>Automatic Feature Learning</h2>
        </section>
        
        <section>
          <img src="images/faces.png">
          <img class="fragment" src="images/cars.png">
          <img class="fragment" src="images/chairs.png">          
        </section>

        
        <!-- ******************************************************
                           *HOW* DNNS work...
        ******************************************************** -->
        
        <section id="how-dnns-work">
          <h1 class="dim">Outline</h1>
          <ol style="width:70%">
            <li class="dim">Why use deep neural nets (DNNs) for NILM?</li>
            <li>How DNNs work</li>            
            <li class="dim">Three DNN architectures for NILM</li>
            <li class="dim">Data augmentation</li>
            <li class="dim">Results</li>
            <li class="dim">Conclusions &amp; future work</li>            
          </ol>
        </section>

        <section>
          <h1>The Artificial Neuron</h1>
          <img src="images/ArtificialNeuronModel.png">
          <p class="tiny">Image adapted from <a href="https://en.wikibooks.org/wiki/File:ArtificialNeuronModel_english.png">WikiMedia Commons image by Chrislb</a></p>
          <!-- TODO:
          ** D3 (low priority): flow of info through neuron **
          -->
        </section>
        
        <section id="neuralnet">
          <h1>Feed Forward Nets</h1>
          <p class="placeholder"></p>
          <span class="fragment"></span>
        </section>

        <section>
          <h1>Training</h1>
          <!--
          TODO: 
          Simple diagram showing:

                            target ---|
                                      V
          input -> net -> output -> compare
                    ^                 |
                    |                 V
                    \---- adjust weights

          Need to learn the weights and biases.
          With 100M learnable parameters, it's not tractable to do an 
          exhaustive search or a random search.
          So use gradient descent.
          Back prop.
          ** D3 (low priority): gradient descent & back-prop **
          -->
        </section>
        
        <section id="autoencoder">
          <h1>Autoencoders</h1>
          <p class="placeholder"></p>
          <span class="fragment"></span>
        </section>

        <section>
          <h2>Autoencoder Examples</h2>
          <div style="height: 600px;">
          <img class="fragment current-visible"
               style="position:absolute; transform: translate(-50%, 0);"
               src="images/HintonAE.png" height=560>
          <img class="fragment current-visible"
               style="position:absolute; transform: translate(-50%, 50%);"               
               src="images/HintonAEexamples.png" width=1100>
          </div>
          <p class="tiny">Hinton &amp; Salakhutdinov. <strong>Reducing the
          dimensionality of data with neural
          networks.</strong> <em>Science</em> (2006)</p>
        </section>
        
        <section>
          <h2>Autoencoder Examples</h2>
          <img src="images/learned_features.gif" height=550>
          <p class="tiny">Animation by <a href="http://www.cs.toronto.edu/~ranzato/research/projects.html#sparse_coding">Marc'Aurelio Ranzato</a></p>
        </section>

        <section>
          <h1>Denoising Autoencoders</h1>
          <img src="images/peppers.png">
          <p class="tiny">Image from <a href="http://www.cs.toronto.edu/~ranzato/research/projects.html">Marc'Aurelio Ranzato</a></p>          
          <p class="tiny">Vincent <em>et al.</em> <strong>Extracting and composing robust features
          with denoising autoencoders.</strong> <em>ICML</em> (2008)</p>
        </section>
        
        <section id="recurrent">
          <h1>Recurrent Neural Nets</h1>
          <p class="placeholder"></p>
          <span class="fragment"></span>
          <span class="fragment"></span>          
        </section>

        <section>
          <h1>Recurrent Neural Nets</h1>
          <h2>Long Short-Term Memory (LSTM)</h2>
          <!-- 
          *** D3 (low priority): LSTM animation ***
          -->
        </section>
        
        <section>
          <h1>Recurrent Neural Nets</h1>
          <h2>Examples</h2>
          <!-- Great for *sequences*. 
          ASR, machine translation, handwriting recognition -->
        </section>

        <!-- ******************************************************
                           Three DNN ARCHITECTURES FOR NILM
        ******************************************************** -->
        
        <section id="dnn-architectures-for-nilm">
          <h1 class="dim">Outline</h1>
          <ol style="width:70%">
            <li class="dim">Why use deep neural nets (DNNs) for NILM?</li>
            <li class="dim">How DNNs work</li>            
            <li>Three DNN architectures for NILM
              <ol>
                <li>Recurrent Neural Nets (LSTM)</li>
                <li>Denoising Autoencoder</li>
                <li>'Bounding rectangle' around the target</li>
              </ol>
            </li>
            <li class="dim">Data augmentation</li>            
            <li class="dim">Results</li>
            <li class="dim">Conclusions &amp; future work</li>            
          </ol>
        </section>        

        <section id="rnns-for-nilm">
          <h1>Recurrent Neural Nets</h1>
          <p class="placeholder"></p>
          <span class="fragment"></span>
          <span class="fragment"></span>
          <span class="fragment"></span>
          <span class="fragment"></span>          
          <!-- 
          ***** D3 (low priority) ******
          Highlight the connections as data flows through, especially
          RNN connections.
          -->
        </section>

        <section id="autoencoder-for-nilm">
          <h1>Denoising Autoencoders</h1>
          <p class="placeholder"></p>
          <span class="fragment"></span>
          <span class="fragment"></span>          
          <span class="fragment"></span>
          <!-- 
          ***** D3 (low priority) ******
          Highlight the connections as data flows through.
          -->
        </section>

        <section id="rectangles-for-nilm">
          <h1>Bounding Rectangle</h1>
          <p class="placeholder"></p>
          <span class="fragment"></span>
          <span class="fragment"></span>
          <span class="fragment"></span>          
          <!-- 
          ***** D3 (low priority) ******
          Highlight the connections as data flows through.
          -->
        </section>


        <!-- ******************************************************
                           DATA AUGMENTATION
        ******************************************************** -->
        
        <section>
          <h1 class="dim">Outline</h1>
          <ol style="width:70%">
            <li class="dim">Why use deep neural nets (DNNs) for NILM?</li>
            <li class="dim">How DNNs work</li>            
            <li class="dim">Three DNN architectures for NILM</li>
            <li>Data augmentation</li>
            <li class="dim">Results</li>
            <li class="dim">Conclusions &amp; future work</li>            
          </ol>
        </section>        

        <section>
          <h1>Data Augmentation</h1>
          <!-- TODO:
          Examples from vision (pics from plankton?) and ASR.
          -->
        </section>

        <section>
          <h1>Data Augmentation</h1>
          <!-- TODO:
          *** D3 (medium priority): illustrate data augmentation ***
          Maybe just a flow chart: start with real appliance-level
          data, extract activations into a set per appliance type,
          etc...
          Show several examples?
          -->
        </section>



        <!-- ******************************************************
                                   RESULTS
        ******************************************************** -->
        
        <section>
          <h1 class="dim">Outline</h1>
          <ol style="width:70%">
            <li class="dim">Why use deep neural nets (DNNs) for NILM?</li>
            <li class="dim">How DNNs work</li>            
            <li class="dim">Three DNN architectures for NILM</li>
            <li class="dim">Data augmentation</li>
            <li>Results</li>
            <li class="dim">Conclusions &amp; future work</li>            
          </ol>
        </section>        

        <section>
          <h1>Example Outputs</h1>
          <!-- TODO:
          *** D3 (high priority): ***
          plot aggregate, ground truth, and estimates from all classifiers.
          -->
        </section>

        <section>
          <h1>Metrics</h1>
          <!-- TODO:
          *** D3 (medium priority): ***
          Start without D3: just show zooms of figure in paper.
          -->
        </section>
        

        <!-- ******************************************************
                                   CONCLUSIONS
        ******************************************************** -->
        
        <section>
          <h1 class="dim">Outline</h1>
          <ol style="width:70%">
            <li class="dim">Why use deep neural nets (DNNs) for NILM?</li>
            <li class="dim">How DNNs work</li>            
            <li class="dim">Three DNN architectures for NILM</li>
            <li class="dim">Data augmentation</li>
            <li class="dim">Results</li>
            <li>Conclusions &amp; future work</li>            
          </ol>
        </section>        

        <section>
          <h1>Conclusions &amp; Future Work</h1>
          <!-- TODO: from paper -->
        </section>

      </div>

    </div>

    <!-- REVEAL -->
    <script src="bower_components/reveal.js/lib/js/head.min.js"></script>
    <script src="bower_components/reveal.js/js/reveal.js"></script>

    <!-- D3 -->
    <script src="bower_components/d3/d3.js"></script>

    <!-- REVEAL TO D3 -->
    <script src="js/reveal-to-d3-config.js"></script>
    <script src="js/reveal-to-d3.js"></script>
    <script src="js/d3-common.js"></script>    
    <script src="js/d3-plot-power-data.js"></script>
    <script src="js/d3-plot-neural-nets.js"></script>    
    
    <!-- REVEAL INIT -->
    <script src="js/reveal-init.js"></script>
    
  </body>
</html>
